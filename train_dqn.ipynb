{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "cX5Nt4qZTjuI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0UXBYECTC7E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "\n",
        "from collections import deque\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Check GPU Available\n",
        "\n"
      ],
      "metadata": {
        "id": "8P9dzN6xTn6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "SJkwpykvYZ4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DQN-RL"
      ],
      "metadata": {
        "id": "OZ4R0jTxU92d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input state dataset"
      ],
      "metadata": {
        "id": "rMn4AktfTwAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ldnMNUETC7H"
      },
      "outputs": [],
      "source": [
        "# state = np.array([[1746, 2855,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0, 3185,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0, 2445,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0, 2095, 4087,    0,    0, 2137,\n",
        "#         2298,    0, 4019, 3319, 2944, 2287, 2256, 3961],\n",
        "#        [   0,    0, 2450, 2291, 1685, 1310,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0],\n",
        "#        [   0,    0,    0,    0,    0,    0,    0,    0, 1197,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0],\n",
        "#        [   0,    0,    0,    0,    0,    0, 3106, 2371,    0, 3820, 3914,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0, 2638, 1316, 3201,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0],\n",
        "#        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0, 3733,    0, 3915,    0, 2785,\n",
        "#         2196,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#         1989, 2314, 2113, 2288,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0],\n",
        "#        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#         1465, 1197, 2069, 3786, 3525, 1061, 2362, 2009, 3871, 4201, 1141,\n",
        "#         3840,    0, 1426,    0,    0,    0,    0, 2290,    0,    0,    0,\n",
        "#            0, 3649, 4097, 1732, 3967, 2636, 2902, 2742,    0, 1201, 2094,\n",
        "#            0,    0,    0,    0, 1272, 3283,    0,    0, 1997, 2158,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0],\n",
        "#        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0, 2834,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0],\n",
        "#        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "#            0, 3708,    0,    0,    0,    0,    0,    0]])\n",
        "\n",
        "# state_init = copy.deepcopy(state)\n",
        "# action_space = (state.shape[0]*state.shape[1]) - 1\n",
        "\n",
        "# packet_amount = {'0': 1738, '1': 2754, '3': 2508, '4': 2343, '5': 1769, '7': 1409, '8': 3022, '9': 2369, '10': 1221, '12': 3664, '14': 3904, '15': 1535, '16': 1113, '17': 1953, '18': 3712, '19': 3618, '20': 1123, '21': 2436, '22': 2081, '23': 4026, '24': 4138, '25': 1166, '26': 3870, '27': 2816, '28': 1478, '29': 2545, '30': 1330, '31': 3224, '32': 3672, '33': 2142, '34': 3963, '36': 3240, '38': 2851, '39': 2232, '40': 3784, '41': 3958, '42': 1722, '43': 3973, '44': 2606, '45': 2919, '46': 2791, '47': 2500, '48': 1211, '50': 2098, '51': 1934, '52': 2304, '53': 2094, '55': 2361, '56': 1233, '57': 3307, '58': 2010, '59': 4108, '60': 1957, '61': 2132, '63': 2177, '64': 2407, '66': 3672, '67': 4148, '68': 3453, '69': 2941, '70': 2311, '71': 2285, '72': 3891}\n",
        "# controller_packet_amount_copy = {'2': 39963, '6': 8029, '11': 1221, '13': 20058, '35': 21411, '49': 68082, '54': 2816, '65': 3672}\n",
        "# capacity = 30000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = np.array([[   0,    0,    0,    0,    0, 1699],\n",
        "                  [2326, 2568, 2103, 3457, 1976,    0],\n",
        "                  [   0,    0,    0,    0,    0,    0]])\n",
        "\n",
        "# state = state.astype('float32')\n",
        "state_init = copy.deepcopy(state)\n",
        "packet_amount = {'2': 2326, '4': 2568, '5': 2103, '6': 3457, '7': 1976, '9': 1699}\n",
        "capacity = 6_000\n",
        "action_space = (state.shape[0]*state.shape[1]) - 1"
      ],
      "metadata": {
        "id": "qVO0pzKqswUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sA = sparse.csr_matrix(state)"
      ],
      "metadata": {
        "id": "JcQMkOF-qAnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ],
      "metadata": {
        "id": "ei9xokpuTz_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8J0n371TC7I"
      },
      "outputs": [],
      "source": [
        "def build_neural_network(state):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Given the state shape, consider rows as timesteps and columns as features\n",
        "    timesteps = state.shape[0]\n",
        "    features = state.shape[1]\n",
        "\n",
        "    # LSTM layer\n",
        "    # model.add(LSTM(128, input_shape=(timesteps, features), return_sequences=True))\n",
        "    # model.add(LSTM(128))\n",
        "    model.add(CuDNNLSTM(64, input_shape=(timesteps, features), return_sequences=True))\n",
        "    model.add(CuDNNLSTM(64))\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(timesteps * features + 1))\n",
        "\n",
        "    model.compile(optimizer=RMSprop(), loss=Huber())\n",
        "    return model\n",
        "\n",
        "online_network = build_neural_network(state)\n",
        "target_network = build_neural_network(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5ruEx8qTC7J"
      },
      "outputs": [],
      "source": [
        "online_network.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Parameters of model training\n",
        "\n"
      ],
      "metadata": {
        "id": "Uj6UNeqiT2OT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f933HfwRTC7K"
      },
      "outputs": [],
      "source": [
        "memory_size = 10_000\n",
        "reward_discount_factor = 0.2\n",
        "number_of_ep = 200\n",
        "epsilon = 1.0\n",
        "epsilon_decay_factor = 0.999\n",
        "epsilon_min = 0.01\n",
        "batch_size = 32\n",
        "target_network_update_int = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Environtment setup instruction"
      ],
      "metadata": {
        "id": "ZI7Tuvv4T5tt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zux646k4TC7K"
      },
      "outputs": [],
      "source": [
        "def migratefromAction(action, state, packet_amount):\n",
        "  state = np.copy(state)\n",
        "  controllers = state.shape[0]  #Rows\n",
        "  switches = state.shape[1]     #Column\n",
        "  packet_amount_np = np.array(list(packet_amount.values()))\n",
        "  action = action + 1\n",
        "  target_con = (math.ceil(action/switches))\n",
        "  target_sw = (action%switches)\n",
        "\n",
        "  if (action%switches) == 0:\n",
        "    target_sw = switches\n",
        "\n",
        "  target_con = target_con -1\n",
        "  target_sw = target_sw - 1\n",
        "\n",
        "  # mapping_copy = copy.deepcopy(mapping)\n",
        "  state[state > 0] = 1\n",
        "  state[target_con][target_sw] = 1\n",
        "  for i in range(controllers):\n",
        "    if i != target_con:\n",
        "      state[i][target_sw] = 0\n",
        "  state = np.multiply(state, packet_amount_np)\n",
        "  return state\n",
        "\n",
        "# res = migratefromAction(12, state1, packet_amount)\n",
        "# print(state1)\n",
        "# print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGTTqPvkTC7L"
      },
      "outputs": [],
      "source": [
        "def calculateNet(state, lambda_, etha):\n",
        "  # Calculate CV based on Equation 5\n",
        "  controller_loads = np.sum(state, axis=1)\n",
        "  mu = np.mean(controller_loads)\n",
        "  sigma = np.std(controller_loads)\n",
        "  CV = sigma / mu\n",
        "  net_value = math.exp(lambda_ * etha - (1 - lambda_) * CV)\n",
        "\n",
        "  return net_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2F1cNubTC7L"
      },
      "outputs": [],
      "source": [
        "def calculateReward(state, next_state):\n",
        "  etha = 0.3\n",
        "  lambda_ = 0.7\n",
        "\n",
        "  net_lastcon = calculateNet(state, lambda_, etha)\n",
        "  net_nowcon = calculateNet(next_state, lambda_, etha)\n",
        "  delta_net = net_nowcon - net_lastcon\n",
        "  if delta_net > 0:\n",
        "    r = 1\n",
        "  elif delta_net == 0:\n",
        "    r = -0.1\n",
        "  else:\n",
        "    r = -10\n",
        "\n",
        "  return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ_s1kLnTC7N"
      },
      "outputs": [],
      "source": [
        "def checkDone(next_state, capacity):\n",
        "  row_sums = np.sum(next_state, axis=1)\n",
        "  done = not np.any(row_sums > capacity)\n",
        "  return done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biqJFVCnTC7N"
      },
      "outputs": [],
      "source": [
        "def step(action, state, packet_amount, capacity):\n",
        "\n",
        "  next_state = migratefromAction(action, state, packet_amount)\n",
        "  reward = calculateReward(state, next_state)\n",
        "  done = checkDone(next_state, capacity)\n",
        "\n",
        "  return next_state, reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "bFhh_tQHT-aV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTnhcNlFTC7O"
      },
      "outputs": [],
      "source": [
        "## สร้าง List เอาไว้เก็บประวัติ reward รวมของแต่ละ ep เพื่อดูการพัฒนา\n",
        "reward_hist = []\n",
        "t = 0\n",
        "## สร้าง memory หรือ D\n",
        "memory = deque(maxlen=memory_size)\n",
        "for ep in tqdm(range(number_of_ep+1)):\n",
        "  #Initial state\n",
        "  state = state_init\n",
        "  reward_of_ep = 0\n",
        "  while True:\n",
        "    ## เลือก action\n",
        "    optimal_action = np.argmax(online_network.predict(state.reshape(1, state.shape[0], state.shape[1], 1))[0])\n",
        "    random_action = random.randint(0,action_space)\n",
        "    action = np.random.choice([random_action,optimal_action],p=[epsilon,1-epsilon])\n",
        "    ## ทำ action ใส่ environment\n",
        "    next_state,reward,done = step(action, state, packet_amount, capacity)\n",
        "    ## เก็บ experience ลงใน memory\n",
        "    memory.append([state,action,reward,next_state,done])\n",
        "    ## รอให้เก็บ experience ให้ได้ตาม batch size ก่อน\n",
        "    if len(memory)>=batch_size:\n",
        "      ## Sample experience จาก memory\n",
        "      batch = np.array(random.sample(memory, batch_size))\n",
        "      ## หาค่า target\n",
        "      target = batch[:,2] + reward_discount_factor*(np.max(target_network.predict(np.stack(batch[:,3])),axis=1)) * (1-batch[:,4])\n",
        "      ## หาค่าพรีดิกในปัจจุบัน\n",
        "      current_Q = target_network.predict(np.stack(batch[:,0]))\n",
        "      ## สร้าง target value list\n",
        "      current_Q[np.arange(batch_size),list(batch[:,1])] = target\n",
        "      ## Fit model\n",
        "      online_network.fit(np.stack(batch[:,0]), current_Q, verbose=False)\n",
        "    ## ก้าวไป next state\n",
        "    state = next_state\n",
        "    ## ลดค่า epsilon\n",
        "    if epsilon>epsilon_min: epsilon = epsilon*epsilon_decay_factor\n",
        "    ## รวมค่า reward_of_ep (เอาไว้ดูเฉยๆ)\n",
        "    reward_of_ep += reward\n",
        "    ## อัพเดท target network\n",
        "    if t%target_network_update_int==0:\n",
        "      target_network.set_weights(online_network.get_weights())\n",
        "    if done:\n",
        "      break\n",
        "    t += 1\n",
        "  reward_hist.append(reward_of_ep)\n",
        "  ## Save model เอาไว้วัดผลในภายหลัง\n",
        "  if ep%20==0:\n",
        "    online_network.save('/content/dqn/model/{}.h5'.format(ep))\n",
        "    pickle.dump(reward_hist,open('/content/dqn/score.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing"
      ],
      "metadata": {
        "id": "3TFGCQSLUB9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I03L2InTC7P"
      },
      "outputs": [],
      "source": [
        "online_network = load_model('/content/dqn/model/{}.h5'.format(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfetqeTSTC7P"
      },
      "outputs": [],
      "source": [
        "state = state_init\n",
        "print(state, packet_amount, capacity)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Action predicition\n",
        "\n"
      ],
      "metadata": {
        "id": "u_dAI0vjUHR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68xLEQPKTC7P"
      },
      "outputs": [],
      "source": [
        "reward_of_ep = 0\n",
        "list_action = []\n",
        "state = state_init\n",
        "while True:\n",
        "  ## เลือก action\n",
        "  optimal_action = np.argmax(online_network.predict(state.reshape(1, state.shape[0], state.shape[1], 1))[0])\n",
        "  ## ทำ action ใส่ environment\n",
        "  next_state,reward,done = step(optimal_action, state, packet_amount, capacity)\n",
        "  ## รวมค่า reward_of_ep (เอาไว้ดูเฉยๆ)\n",
        "  reward_of_ep += reward\n",
        "  ## ก้าวไป next state\n",
        "  state = next_state\n",
        "  list_action.append(optimal_action)\n",
        "  if done:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   List of predicted action\n",
        "\n"
      ],
      "metadata": {
        "id": "uWGJ6sqMU37C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5SpdmI_TC7Q"
      },
      "outputs": [],
      "source": [
        "print(list_action)\n",
        "print(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Reward history\n",
        "\n"
      ],
      "metadata": {
        "id": "Kz6VytolULvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_hist = pickle.load(open('/content/dqn/score.pkl','rb'))\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(reward_hist)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')"
      ],
      "metadata": {
        "id": "0re_Y2JrUPZn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}